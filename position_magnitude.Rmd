---
title: 'Top of the Charts: The Influence of Position and Axis Range on Perceived Magnitude'
author: "Duncan Bradley"
date: "14/02/2022"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Seed for random number generation
set.seed(45789)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

# Loading packages
library(papaja) 
library(tidyverse) 
library(ordinal) 
library(patchwork)
library(magick) 
library(emmeans) 
library(egg) 
library(scales) 
library(buildmer) 
library(lme4)
library(broom)
```

```{r load-data, include=FALSE}
risk1_anon <- read_csv("data/risk1_anon.csv")
risk2_anon <- read_csv("data/risk2_anon.csv")
risk3_anon <- read_csv("data/risk3_anon.csv")
```

```{r wrangle, include=FALSE}
wrangle <- function(anon_file) {

# extract literacy data
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = mean(c(q1_slider.response, 
                           q2_slider.response, 
                           q3_slider.response, 
                           q4_slider.response, 
                           q5_slider.response))) %>%
    select(participant,
         literacy)

edu_labels <- set_names(c('No formal qualications',
                'Secondary education (e.g. GED/GCSE)',
                'High school diploma/A-levels',
                'Technical/community college',
                'Undergraduate degree (BA/BSc/other)',
                'Graduate degree (MA/MSc/MPhil/other)',
                'Doctorate degree (PhD/other)',
                'Don\'t know / not applicable'),
          seq(8,1,-1))

# extract demographics

  demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response, 
                                         `1` = "F", 
                                         `2` = "M", 
                                         `3` = "NB")) %>%
  mutate(across(matches("edu_slider.response"),
                ~recode(edu_slider.response, !!!edu_labels))) %>%
      select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response",
                   "edu_slider.response")))
  
  anon_file %>% 
  select(matches(c("participant", 
                   "item_no",
                   "condition",
                   "pos",
                   "orientation",
                   "chance_slider.response",
                   "severity_slider.response",
                   "chance_slider.rt",
                   "severity_slider.rt",
                   "data_mean",
                   "key_resp.rt",
                   "type"))) %>% 
      filter(type == "E") %>%
        inner_join(literacy, by = "participant") %>%
    inner_join(demographics, by = "participant") %>%
      mutate(across(matches(c("pos", "orientation", "condition")), as_factor)) %>%
  mutate(across(c("chance_slider.response",
                  "severity_slider.response"), as.ordered)) %>%
  mutate(across(c("participant",
                  "item_no"), as.character)) %>%
  rename("ori"= matches("orientation")) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
         value = ., envir = .GlobalEnv)
}

walk(list(risk1_anon,
         risk2_anon,
         risk3_anon),
    wrangle)

contrasts(risk1_tidy$condition) <- matrix(c(.5, -.5))
contrasts(risk2_tidy$ori) <- matrix(c(.5, -.5))
contrasts(risk2_tidy$pos) <- matrix(c(.5, -.5))
contrasts(risk3_tidy$condition) <- matrix(c(.5, -.5))
```

```{r fix-age-typo, include=FALSE}
# age typo
max(risk2_tidy$age_textbox.text)

# finding session number for this participant
age188session <- risk2_anon %>% filter(age_textbox.text == 188) %>% pull(session)

# using session code to find true age in csv of demographic data from prolific (prolific_export.csv)
# we are unable to include this csv as it contains Prolific participant IDs
# prolific_export %>%
#   filter(session_id == age188session) %>% pull(age)
# [1] 18
risk2_tidy <- risk2_tidy %>% 
  mutate(age_textbox.text = recode(age_textbox.text, `188` = 18))

max(risk2_tidy$age_textbox.text)
```

```{r, comparison-function, include=FALSE}
comparison <- function(model, remove = NULL) {
  
  form <- formula(model)
  
  reducefixed <- function(form) {
    
    fixedfx <- 
      remove.terms(form,"placeholder") %>% # generate full formula (expand '*')
      nobars() # get formula for fixed effects only
    
    fixedterms <-  
      terms.formula(fixedfx) %>% # get terms for fixed effects
      attr("term.labels") # get character vector of fixed effects terms
    
    out <- remove.terms(fixedfx, tail(fixedterms, n=1))
    
    # remove will only take a single character string, not a character vector
    if(!is.null(remove))
      out <- remove.terms(fixedfx, remove)
    
    return(out)
  }
  
  getrandom <- function(form) {
    
    parens <- function(x) {paste0("(",x,")")}
    onlyBars <- function(form) {
      reformulate(
        sapply(
          findbars(form), # list of charcater vector for each random effect
          function(x)  parens(deparse(x))), # put each character vector in brackets
        response = form[[2]]) 
    }
    
    out <- onlyBars(form)
    return(out)
  }
  
  merge.formula <- function(form1, form2, ...){
    # adapted from https://stevencarlislewalker.wordpress.com/2012/08/06/merging-combining-adding-together-two-formula-objects-in-r/
    
    # get character strings of the names for the responses 
    # (i.e. left hand sides, lhs)
    lhs1 <- deparse(form1[[2]])
    #print(lhs1)
    lhs2 <- deparse(form2[[2]])
    #print(lhs2)
    if(lhs1 != lhs2) stop('both formulas must have the same response')
    
    # get character strings of the right hand sides
    rhs1 <- strsplit(paste(form1[3]), " \\+ ")[[1]] 
    rhs2 <- strsplit(paste(form2[3]), " \\+ ")[[1]] 
    
    # put the two sides together with the amazing 
    # reformulate function
    out <- reformulate(termlabels = c(rhs1, rhs2), 
                       response = lhs1)
    
    # set the environment of the formula (i.e. where should
    # R look for variables when data aren't specified?)
    #environment(out) <- parent.frame()
    return(out)
  }
  
  newfixedfx <- reducefixed(form)
  fullranfx <- getrandom(form)
  merge.formula(newfixedfx, fullranfx)
  
}
```

```{r, anova-results-function, include=FALSE}
anova_results <- function(model, cmpr_model) {
  
  # first argument 
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer")  model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
      
  anova_output <- ordinal:::anova.clm(model, cmpr_model)
  # use of ordinal:::anova.clm based on https://github.com/runehaubo/ordinal/issues/38  
  
  assign(paste0(model_name, ".LR"),
         anova_output$LR.stat[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
}
```

# Introduction

Context is crucial for effectively judging the magnitude of numbers. It is indisputable that a 40% probability is twice as great as a 20% probability, but in the absence of context, it is unclear whether this value should be considered large or small.

```{r nyt-chart, fig.cap= "New York Times Data Visualisation", echo=FALSE}
knitr::include_graphics("figures/nyt-chart.png")
```

In charts, numerical axes often provide some context for judging the magnitude of plotted values. The range of values on an axis provides a frame of reference for assessing whether a data point is large or small. A bar chart produced by the New York Times, which plots over time the number of black members of the U.S. senate, provides a striking illustration (Figure \@ref(fig:nyt-chart)). Unusually, the continuous y-axis does not terminate just above the highest value. Instead, it extends all the way to the total number of senators: 100. As a result, bars representing black senators are constrained to the very bottom, visible just above the x-axis, and a significant expanse of blank space looms above them. This highlights the absent data points: the vast majority of senators who are not black. The contrast communicates the magnitude of the plotted values in context. 

It is unclear exactly how an axis range influences inferences about magnitude. One possible explanation is that the unfilled area of absent values provides context. That is, plotted values may be judged as small in magnitude because the potential for substantially larger values is clearly displayed. Alternatively, assessments of magnitude may be influenced by the appearance of plotted values only, and not by contrast with blank space. Graphical elements that encode values appear lower down in a chart when they are a great distance below the upper axis limit, compared to when they are only a small distance below. So, viewers may simple judge data points at higher positions as 'high' and those lower down as 'low'. This study explores which of these two accounts explains how axis ranges contribute to the communication of magnitude.

## EFFECTS OF CONTEXT ON MAGNITUDE JUDGEMENTS

Empirical evidence demonstrates that judgement of a value's magnitude (how large or small it is) depends on its relationship to a grand total or to surrounding values. This affects interpretation of verbal approximations, and also absolute values. For example, participants instructed to take 'a few' marbles picked up more when the total number available was larger (Borges & Sawyers, 1974) and rated satisfaction with the same salary as higher when it appeared in the upper end of a range, compared to the lower end (Brown, Gardner, Oswald, & Qian, 2008). 

Psycholinguistic research has demonstrated that implied context can affect responses even when no contrasting values are provided. Scalar adverbs like 'only' and 'almost' influence mental models of a situation by situating numerical quantities on different scales, thus assigning different relative positions to a given value (Jarvella, Lundquist, & Hyönä, 1995). Use of scalar adverbs in the resolution of ambiguity demonstrates their influence on inferences about values. In Lundquist and Jarvella (1994), participants read vignettes containing a scalar adverb, then were asked to interpret ambiguous noun phrase. Participants responded in a manner consistent with the outcome implied by the scalar adverb. That is, a character who gained 'only' 500 votes in an election was assumed to be falling behind, whereas a character with 'almost' 500 votes was assumed to be leading. Even though no other numerical information (such as the maximum number of votes) was supplied, scalar adverbs implied an approximate placement of the given value relative to the maximum.

## AXIS LIMITS: EFFECTS ON COMPARISON

Several recent studies have explored how axis limits can alter impressions of the *relationships between* presented values, rather than the magnitude of the values themselves. When axis ranges are enlarged to create blank space around of cluster of data points, correlation is perceived as stronger (Cleveland, Diaconis, & McGill, 1982). Participants also rate the differences between  values in bar charts as greater when the vertical gap between bars is larger, due to a truncated y-axis (Pandey, Rall, Satterthwaite, Nov, & Bertini, 2015). Correll et al.'s (2020) experiments found that greater truncation resulted in higher effect-size judgements in both line charts and bar charts. Truncation effects persisted even when participants estimated the values of specific data points, suggesting this bias is driven by initial impressions, rather than a misinterpretation of the values portrayed by graphical markings. Correll et al. (2020) found no reduction in effect size judgements when truncation was communicated using graphical techniques (e.g., axis breaks and gradients). The unavoidable consequence, they suggest, is that designers' choices will influence users' interpretations whether the axis is truncated or if it is not.

Choosing an appropriate y-axis range involves a trade-off between bias and sensitivity. Just as a highly truncated y-axis can exaggerate trivial differences between values, an axis spanning the entire range of possible values can conceal important differences (Witt, 2019). Based on participants' judgements of effect size, Witt (2019) found that bias was reduced and sensitivity increased when using an axis range of approximately 1.5 standard deviations of the plotted data, compared to axes which spanned only the range of the data, or the full range of possible values (0-100). This provides further evidence of a powerful association between the appearance of data, when plotted, and subjective assessments of that data. Witt (2019, see also Yang et al., 2021) also argues that any degree of truncation in a bar chart may amount to a more severe misrepresentation of data than in a line chart. When bar length is altered through truncation, the principle of proportional ink is violated, and values are misrepresented, causing an additional distortion that does not affect line charts.

Further evidence of truncation effects, provided by Yang, Vargas Restrepo, Stanley, and Marsh (2021), improves on the design of previous studies which employed few observations (e.g., Pandey et al., 2015; Witt, 2019). Participants' ratings of the difference between two bars provided consistent evidence of the exaggerating effects of y-axis truncation. Yang et al. (2021) discussed various potential mechanisms behind truncation effects and noted that the effect appears to be largely automatic, so may function like an anchoring effect, where a numerical judgement is influence by a reference point (Tversky & Kahneman, 1981). Another explanation draws upon Grice's cooperative principle (Grice, 1975). According to this account of effective communication, speakers are assumed to be in cooperation, and so will communicate in a manner that that is informative, truthful, relevant, and clearly expressed. So a viewer will assume that a difference must be genuinely large if it appears large, else it would not be presented in that way. Effective visualisations will be designed so that a user's instinctive characterisation of the data corresponds closely to their view following a detailed inspection (Yang et al., 2021).

## AXIS LIMITS - EFFECTS ON EXTRACTION

The above research consistently demonstrates that relationships between data points are interpreted differently depending on their appearance when plotted. Our focus is on how interpretations of the magnitude of *the values themselves* are affected by their visual features in charts. Evidence suggests that vertical position is a strong index of magnitude with a substantial influence within human cognition. For example, children appear to intuitively understand the relationship between height and value (Gattis, 2002). Both the physical world, and language (e.g., spatial metaphors), provide countless examples where 'higher' is associated with 'more', and 'lower' with 'less', and this principle has been adopted as a convention in data visualisation (Tversky, 1997). The strength of this association is illustrated by research showing how inversions of this mapping in visualisations can lead to misinterpretations (Okan, Garcia-Retamero, Galesic, & Cokely, 2012; Pandey et al., 2015, Woodin et al., 2021).

In addition to a general role in comprehension, there is also evidence suggesting that physical position plays a part in magnitude judgements. When a company's financial performance was displayed entirely in the bottom fifth of a line chart, the company was perceived as less successful than when no blank space appeared above the maximum value (Taylor & Anderson, 1986). Sandman et al., (1994) investigated assessments of magnitude in risk ladders, where greater risks are physically higher on a vertical numerical scale. To assist evaluation of risk magnitudes for asbestos exposure (a less familiar hazard), a scale for smoking frequency (a familiar hazard) was presented alongside for reference. Participants rated the threat of asbestos higher when it was plotted at a higher position. 

These studies can be taken as preliminary evidence that changing axis limits affects appraisals of data points' magnitudes, but have several issues. Taylor and Anderson (1986) did not report how judgements were elicited or their sample size. Sandman et al. (1994) only explored responses towards one specific risk (asbestos), each participant only took part in a single trial. In addition, their response variable combines several separate ratings, which prevents diagnosis of whether manipulations affected interpretations of the plotted information in particular, or just related concepts. Further, both studies introduced a confounding variable by adjusting the difference between the minimum and maximum y-axis values across conditions. To understand how different displays of the same data elicit different inferences, and to provide recommendations for best practice, stronger evidence and investigation into cognitive mechanisms involved, are necessary. 

## The Present Study

In a set of three experiments, we investigate how employing different axes limits affects interpretations of magnitude. This manipulation changes both the numerical context surrounding data points, and their physical positions, but the data points themselves remain the same.

All data visualisations used in the present set of experiments displayed the probability of negative events. This context provides participants with a purpose: evaluating information in risk scenarios is a more meaningful task than assessing, in the abstract, how 'large' a value is. Furthermore, charts are frequently used for risk communication, and research suggests that manipulating aspects of a chart can change perceptions of the risks displayed (Elting, Martin, Cantor, & Rubenstein, 1999; Feldman-Stewart, Kocovski, McConnell, Brundage, & Mackillop, 2000; Keller, Siegrist, & Visschers, 2009; Okan, Stone, Parillo, Bruine de Bruin, & Parker, 2020; Zikmund-Fisher, Fagerlin, & Ubel, 2005).

Risk events are comprised of two core components: 1) probability of occurrence and 2) severity of harm. However, individuals' assessments of probability and severity are not necessarily independent. When the same event is described as having more severe outcomes, it is perceived as more likely (Harris & Corner, 2011, Harris et al., 2009). In a similar manner, when the same event is described as more probable, it is associated with more substantial consequences (Kupor & Laurin, 2020). Perceptions of probability and outcome magnitude are related because they are both assumed to reflect the potency of the event's cause (probability-outcome correspondence principle; Keren & Teigen, 2001). According to this account, probabilities can occasionally provide meaningful indications of magnitude (e.g., rainfall), but it is inappropriate to apply this perspective to all scenarios. 

Risk events are comprised of two core components: 1) probability of occurrence and 2) severity of harm. However, individuals' assessments of probability and severity are not necessarily independent. When the same event is described as having more severe outcomes, it is perceived as more likely (Harris & Corner, 2011, Harris et al., 2009). In a similar manner, when the same event is described as more probable, it is associated with more substantial consequences (Kupor & Laurin, 2020). One account suggests that perceptions of probability and outcome magnitude are related because they are both assumed to reflect the potency of the event's cause (probability-outcome correspondence principle; Keren & Teigen, 2001). According to this account, probabilities can occasionally provide meaningful indications of magnitude (e.g., rainfall), but it is inappropriate to apply this perspective to all situations. Therefore, even though charts in the present set of experiments only display outcome probability, assessments of outcome severity may also differ between conditions. Collecting separate judgements of outcome probability and outcome severity for each scenario will provide a clearer picture of the effects of the manipulation on each aspect of participants' representations of risk. Use of Likert scales (with discrete options) rather than visual analogue scales (with continuous options; Sung & Wu, 2018) will prevent participants from simply mapping probability percentages directly onto a linear scale. We also collect a data visualisation literacy measure, in order to test whether differences in this capacity can account for differences in the degree to which the range of axis values affect individuals' interpretations. Individuals with lower graph literacy are more susceptible to generating biased interpretations of visualisations which violate graphical conventions by using atypical scales (Okan, Galesic, & Garcia-Retamero, 2016; Okan et al., 2012). These individuals are more likely to make inferences about data points' values by drawing on their physical positions (Okan et al., 2016). 

# Experiment 1

## Introduction

Our initial experiment attempts to establish whether changing the axes limits in a chart does indeed affect interpretations of the magnitude of data points. We presented the same data points at different vertical positions by altering both the upper and lower y-axis limits.

We predicted that probability magnitude ratings and/or severity magnitude ratings would be greater when data points were presented at higher positions, compared to when the same data points were presented at lower positions.

## Methods

### Participants

```{r r1_demographics, echo=FALSE}
age_r1 <- distinct(risk1_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(age_textbox.text, na.rm = TRUE), sd = sd(age_textbox.text, na.rm = TRUE)) 

gender_r1 <- distinct(risk1_tidy, participant, .keep_all = TRUE) %>% group_by(gender_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% pivot_wider(names_from = gender_slider.response, values_from = perc)

literacy_r1 <- distinct(risk1_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(literacy, na.rm = TRUE), sd = sd(literacy, na.rm = TRUE))
```

A total of 161 participants were recruited through Prolific.co, a platform for recruiting participants for online studies. Normal or corrected-to-normal vision and English fluency were required for participation. Per pre-registered rejection criteria[^1], 10 participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly[^2], and one participant's submission was rejected because they did not complete the experiment, so no data were available for this participant. This left a total of 150 participants whose submissions were accepted (`r printnum(gender_r1$M)`% male, `r printnum(gender_r1$F)`% female, `r printnum(gender_r1$NB)`% non-binary). Mean age was `r printnum(age_r1$mean)` (*SD* = `r printnum(age_r1$sd)`)[^3]. Graph literacy scores consisted of the sum of responses on five six-point scales. The mean graph literacy score was `r printnum(literacy_r1$mean)` (*SD* = `r printnum(literacy_r1$sd)`). Participants whose submissions were approved were paid £3.55.

[^1]: <https://osf.io/23wpn>

[^2]: Attention check questions are described in the Procedure.

[^3]: Age data failed to save for one participant, but was available for all other 149 participants whose submissions were accepted.

### Materials

Text and an accompanying chart were presented in each trial. Two sentences outlined a scenario involving a risk, and explained what the chart depicted. For example:

*You are going on a camping trip next week. The graph below shows the chance of heavy rainfall for three randomly selected days next week.*

Each dot plot displayed the probability (as a percentage) of a negative outcome occurring, for three options associated with the scenario (Figure \@ref(fig:example-charts)). The label 'Chance' was used instead of 'Probability' to avoid confusion with the standard 0-1 scale for probabilities, and to reflect casual usage.

```{r example-charts, fig.cap= "Example Charts", echo=FALSE}
img1 <- image_read("figures/E23_hi.png")
img2 <- image_read("figures/E23_lo.png")
image_append(c(img1, img2))
```

Each dataset was generated from a Gaussian distribution. In experimental trials (n = 40), all three data points were either plotted in the top third of the chart ('high' condition: Figure \@ref(fig:example-charts), left) or in the bottom third of the chart ('low' condition: Figure \@ref(fig:example-charts), right). The dataset differed for each scenario, but was identical for the two conditions associated with each scenario. In filler trials (n = 15) and attention check trials (n = 5), all three data points were plotted in the middle third of the chart.

The y-axis range in each chart was 10 percentage points. Horizontal gridlines appeared at one-unit increments. In all trials, the gridline 1.5 percentage points above the bottom edge of the chart was labelled with a percentage value, as was the gridline 1.5 percentage points below the top edge of the chart.

### Procedure

The experiment was programmed in PsychoPy (Peirce, 2019) and hosted on pavlovia.org. Participants were informed that they should complete the experiment on a desktop computer or laptop, not a tablet or mobile phone. After providing informed consent, participants submitted their age and gender, and completed a five-item graph literacy scale (Garcia-Retamero et al., 2016). They were reminded that the experiment involved information about risks, which could cause distress, so were entitled to withdraw from the experiment. Following this, instructions explained that the task involved assessing the probability and severity of negative outcomes in various scenarios involving risks. The instructions noted that some scenarios might appear similar to other scenarios. Participants were asked to complete the task as quickly and accurately as possible. Two practice trials were presented before the experiment proper began.

Two responses were required for each trial: one rating for the probability of the negative event occurring; and one rating of the severity of the consequences if the negative event occurred. Above the Likert scales, a short phrase indicated that the questions should be answered in response to the plotted data (e.g., *"If you camp on one of these days..."*).

Each Likert scale had two anchors at the extremes, but all other options were unlabelled. The leftmost option in the 'probability' Likert scale was *'Very unlikely'* and the rightmost option *'Very likely'*. The leftmost option in the 'severity' Likert scale was *'Very mild'* and the rightmost option *'Very severe'*. Likert scales appeared on the same screen as the text and chart (see Figure \@ref(fig:example-trial)). Participants were permitted to change their responses as many times as they wished before proceeding to the next trial, but could not return to previous trials.

```{r example-trial, fig.cap= "Example Trial", echo=FALSE}
knitr::include_graphics("figures/example_trial.png")
```

Attention check trials (n = 5) followed the same layout, with text, a chart, and Likert scales, but the task differed. Participants were instructed not to attend to the chart, and instead to provide specified responses on the Likert scales. For example:

*You are expected to stay on task throughout this experiment. For this trial, ignore the graph below. Respond 'Very unlikely' on the top scale, and 'Very mild' on the bottom scale.*

For attention check trials, the questions above the Likert scales were as follows:

*What is the chance response specified above?*

*What is the severity response specified above?*

Before exiting the experiment, participants were informed that all data presented was fictional, and were offered guidance in case of any distress.

### Design

The experiment used a within-participants design, with each participant responding to both versions of each chart. The independent variable, vertical position, had two levels: *high* (data points presented in the upper third) and *low* (data points presented in the lower third). In each trial, responses for the two dependent variables were elicited: participants rated the probability and severity of the negative event on seven-point Likert scales.

Materials were divided into two lists. One list contained five experimental items in the high condition, five experimental items in the low condition, eight filler items, and two attention check questions. The other list contained the alternate versions of each of the experimental items, seven different filler items, and three different attention check questions. The order of the two lists was counterbalanced across participants, and within each list, items were presented in a random order.

## Analysis

Analyses were conducted using R and RStudio. Raw data and analysis scripts are available at (link).

Likert scales only express granularity at the level of ordinal data. They record whether one rating is higher or lower than than another, but not the magnitude of this difference. Therefore, Likert scales do not capture values from latent distributions (e.g., mental representation of probability) in a linear manner. On a Likert scale, the distance between one pair of points and another pair may appear equal, but may represent very different distances on the latent distribution. Therefore, it can be inappropriate to analyse Likert scale data with metric models, such as linear regression (Liddell & Kruschke, 2018). Throughout this paper, we construct cumulative link mixed-effects models, using the *ordinal* package (Christensen, 2021) to analyse magnitude ratings.

Selection of random effects structures for experimental models was automated using the *buildmer* package (Voeten, 2022). The maximal structure included by-participant and by-item random intercepts, as well as by-participant and by-item slopes for the fixed effect (e.g., condition; Barr et al., 2012). From this formula, *buildmer* first identified the most complex model which could successfully converge, prioritising the terms which explained the most variance in the data, then eliminated terms which did not provide significant contributions (assessed using likelihood ratio tests).

```{r likert-plot-function, echo=FALSE}
likert_plot <- function(df) {
  
  IV1 <- df %>% select(matches(c("condition", "pos"))) %>% names() %>% as.name()
  
  conds <- df %>% select(matches(c("condition", "pos", "ori"))) %>% names() 
  
  n_obs <- df %>% nrow()/(length(conds)*2)

  df %>% 
    ggplot(aes(y = {{IV1}}, fill = chance_slider.response)) +
    geom_bar(width=0.5,
             position = position_stack(reverse = TRUE)) +
    scale_fill_brewer(type = "div", palette = "RdYlGn",
                      direction = -1) +
    labs(x = "Probability Magnitude Ratings",
         y = NULL) +
    scale_y_discrete(labels=c("Low Position", "High Position"),
                     limits = c("lo", "hi"))  +
    scale_x_continuous(labels=c("\"Very\nunlikely\"",
                                "\"Very\nlikely\""), 
                       breaks = c(0, n_obs),
                       position = "top") +
    geom_segment(aes(x = 0, xend = n_obs, # specifying the arrow
                     y = 2.5,
                     yend = 2.5),
                 arrow = arrow(ends = "both")) + 
    theme_minimal(base_size = 20) +
    theme(axis.text.x = element_text(face = "italic"),
         legend.position = "none") +
    coord_cartesian(clip = "off")
}
```

```{r r1-p-plot, warning = FALSE, echo=FALSE, fig.cap= "Participants rated the probability of each negative event occurring on a 7-point Likert scale. The distribution of ratings, ranging from \"Very unlikely\" (far left, dark green) to \"Very likely\" (far right, red), is shown separately for charts where values were presented at a High Position (top) and a Low Position (bottom). Note that a greater proportion of values in the High Position condition were rated as representing higher probabilities (right-hand side) than values in the Low Position condition, where a greater proportion of values were rated as representing lower probabilities (left-hand side)."}
likert_plot(risk1_tidy)
```

Figure \@ref(fig:r1-p-plot) shows the distribution of probability magnitude ratings for data points presented at high and low positions, across all experimental items.

```{r, r1-p, cache=TRUE, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE}
r1_p <- buildclmm(chance_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = risk1_tidy)
```

```{r r1-p-cmpr, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r1-p"}
r1_p_cmpr <- clmm(comparison(r1_p),
             data = risk1_tidy)
```

```{r r1-p-anova, eval=TRUE, echo=FALSE}
anova_results(r1_p, r1_p_cmpr)
```

A likelihood ratio test reveals a significant difference between the distribution of probability magnitude ratings in the 'high position' and 'low position' conditions: (𝜒^2^)(`r in_paren(r1_p.df)`) = `r printnum(r1_p.LR)`, p `r printp(r1_p.p, add_equals = TRUE)`. Data points at high positions elicited higher ratings, on average, than the same data points at low positions.

```{r r1-s, cache=TRUE, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE}
r1_s <- buildclmm(severity_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = risk1_tidy)
```

```{r r1-s-cmpr, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r1-s"}
r1_s_cmpr <- clmm(comparison(r1_s),
             data = risk1_tidy)
```

```{r r1-s-anova, eval=TRUE, echo = FALSE, dependson="r1-s"}
anova_results(r1_s, r1_s_cmpr)
```

There was also a significant difference between severity magnitude ratings in the 'high position' and 'low position' conditions: (𝜒^2^)`r in_paren(r1_s.df)`) = `r printnum(r1_s.LR)`, p `r printp(r1_s.p, add_equals = TRUE)`. Severity magnitude ratings were higher, on average, when data points representing probabilities were presented at higher physical positions, compared to when they were presented at lower physical positions.

```{r summary-extract-function, echo=FALSE}
summary_extract <- function(model, key_term) {
  
  params <- c("statistic", "p.value")

  model_name <- deparse(substitute(model))
  
  one_row <- tidy(model) %>% filter(term == key_term)

    get_cols <- function(param) {

    assign(value = one_row %>% pull(param),
           envir = .GlobalEnv,
           paste0(model_name, ".", param))
    }

    lapply(params, get_cols)

}
```

```{r r1-pl, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r1-p"}
r1_pl <- clmm(add.terms(formula(r1_p), "literacy"),
              data = risk1_tidy)
```

```{r r1-sl, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r1-s"}
r1_sl <- clmm(add.terms(formula(r1_s), "literacy"),
              data = risk1_tidy)
```

```{r r1-pl-summary, echo=FALSE, include=FALSE}
summary_extract(r1_pl, "condition1")
```

```{r r1-sl-summary, echo=FALSE, include=FALSE}
summary_extract(r1_sl, "condition1")
```

Adjusting for graph literacy scores did not eliminate the effects of data points' positions on probability magnitude ratings (z = `r printnum(r1_pl.statistic)`, p `r printp(r1_pl.p.value, add_equals = TRUE)`) or severity magnitude ratings (z = `r printnum(r1_sl.statistic)`, p `r printp(r1_sl.p.value, add_equals = TRUE)`).

## Discussion

Participants rated the probability and severity of negative events as greater when data points denoting an event's probability were presented near the top of the chart, compared to when the same data points were presented near the bottom. This suggests the positioning of data points affects interpretation of the magnitude of plotted and related values. Modelling graph literacy in addition to condition did not remove the influence of condition on interpretations. 

Higher bars and ascending lines typically represent higher numbers and ascending trends, so within a single chart, inferring that values presented near the top are greater than those near the bottom will often be correct. This experiment, however, establishes that inferences about the magnitude of *the same value* can change depending its position in different charts.

Both outcome probability and outcome severity ratings were affected by the manipulation of axis limits and data points' positions, even though charts only plotted probability information. This mirrors previous reports of an interplay between properties of presented information and impressions of related but distinct concepts, in particular the finding that higher prior probabilities were associated with impressions of greater event magnitudes (Kupor & Laurin, 2020).

It is unclear whether the effects of different axis ranges on interpretations of magnitude are driven by an association between *absolute* position and magnitude, or an association between *relative* position and magnitude. If absolute position influences interpretations, mentally representing the magnitude of a data point may simply involve associating data points at higher positions with higher values (and lower positions with lower values). If relative position influences interpretations, mentally representing the magnitude of a data point would involve a comparison with plausible alternative values, which are not plotted, but implied through use of blank space. This distinction is explored in Experiment 2.

# Experiment 2

## Introduction

E1 found that viewers associate data points with greater magnitude when those data points are positioned near the *top* of a graph and substantial blank space appears *below* them, compared to when the same data points are positioned near the *bottom* of a graph, with substantial blank space *above*.

One explanation for this finding is that participants made simple associations between absolute position and magnitude, equating physically higher data points with greater magnitudes and physically lower data points with lesser magnitudes. This accords with well-established conceptual metaphors for magnitude, where greater vertical position denotes greater numerical magnitude (Tversky, 1997).

An alternative explanation is that participants used blank space as a reference point when assessing the magnitude of plotted values. For example, when viewing substantial blank space above plotted data points, participants may have recognised the potential for larger values than those observed, thus associating the plotted data points with lesser magnitudes.

E1 does not provide means of differentiating these explanations. For each chart, drawing inferences from absolute position would orient magnitude judgements in the same direction as relying on its position relative to blank space. A high magnitude is implied by a data point's high physical position *and* the presence of substantial blank space below. An additional experiment is required in order to distinguish between the two competing explanations. 

Inverting a vertical axis changes the relationship between physical position and numerical value: increasing *lower* positions represent increasingly *higher* numerical values. This means data points presented near the *bottom* of a chart, with substantial blank space above, are numerically *larger* than the plausible values represented by this blank space. However, associations between physical positions and magnitude, based on conceptual metaphors, may still remain. Therefore, inferences involving blank space would generate the opposite impressions compared to inferences involving data points' physical positions only (Figure \@ref(fig:nyt-chart)). In E2, we manipulate data points' physical positions by changing axis limits (as in E1), and also manipulate axis orientation, by employing conventional and inverted axes (2x2 experiment). If interpretations of magnitude differ according to whether data points are near the extreme with smaller or larger axis values, rather than their physical positions, this will demonstrate that interpretations are guided by blank space.

```{r r2-rationale-plot, echo=FALSE, fig.cap= "Rationale for E2: Distinguishing the Roles of Absolute and Relative Position. \n In charts with conventional axis orientations (left column), there is congruity between data points’ absolute positions and their relative positions in the chart. \n In charts with inverted axis orientations (right column), there is incongruity between data points’ absolute positions and their relative positions in the chart. \n For example, at high absolute positions in conventional charts (top left), data points are relatively higher than alternatives. But at the same absolute positions, in inverted charts, the same values are relatively lower than alternatives (top right)."}

# create my theme
my_theme <- function() {
  theme_minimal(base_size = 12) +
    theme(panel.border = element_rect(fill = NA, size = 1),
          panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          axis.title.x = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.y = element_text(size = 12, face = "bold"),
          axis.text.y = element_text(size = 15, colour = "black", face = "bold"),
          plot.title = element_text(face = "bold"),
          aspect.ratio = 4.5/3,
    )
}

x <- c('A','B','C')
y <- c(38, 38.2, 37.8)

df <- as_tibble(cbind(x, y)) %>%
  mutate_at(vars("y"), as.numeric)

data_mean <- 38

id <- expand_grid(c("hi", "lo"), c("conventional", "inverted"))

create_plot <- function(pos, orientation){

  # set upper and lower limits around the population mean of the data
  lower_lim <- case_when(pos == "lo" & orientation == "conventional" ~ data_mean - 1.5, 
                         pos == "hi" & orientation == "conventional" ~ data_mean - 8.5,
                         pos == "lo" & orientation == "inverted" ~ data_mean - 8.5, 
                         pos == "hi" & orientation == "inverted" ~ data_mean - 1.5)
  upper_lim <- case_when(pos == "lo" & orientation == "conventional" ~ data_mean + 8.5, 
                         pos == "hi" & orientation == "conventional" ~ data_mean + 1.5,
                         pos == "lo" & orientation == "inverted" ~ data_mean + 1.5, 
                         pos == "hi" & orientation == "inverted" ~ data_mean + 8.5)
  
  # set the values for other variables
  # y_order = the order for the two y-axis value labels (bottom, top)
  # axis_transform = reverse axis or not
  y_order <- case_when(orientation == "conventional" ~ c(lower_lim, upper_lim),
                       orientation == "inverted" ~ c(upper_lim, lower_lim))
  
  axis_transform <- case_when(orientation == "conventional" ~ "identity",
                              orientation == "inverted" ~ "reverse")
  
  fill_colour <- case_when(orientation == "conventional" ~ "white",
                           orientation == "inverted" ~ "grey")
  
  divider_alpha <- case_when(orientation == "conventional" ~ 1,
                           orientation == "inverted" ~ 0)
  
  comparison_text <- case_when(pos == "lo" & orientation == "conventional" ~ "LOWER", 
                               pos == "hi" & orientation == "conventional" ~ "HIGHER",
                               pos == "lo" & orientation == "inverted" ~ "HIGHER", 
                               pos == "hi" & orientation == "inverted" ~ "LOWER")
  
text_pos <- min(upper_lim, lower_lim) + 
    (max(upper_lim, lower_lim) - min(upper_lim, lower_lim))/2

  arrow_start <- lower_lim +2.5
  arrow_end <- upper_lim -2.5
  
  # create the plot
  g <- df %>% ggplot(aes(x = x,
                         y = y)) + 
    geom_point(size = 4) + 
    ylab("") +
    coord_cartesian(ylim = y_order, 
                    xlim = c(0.5, 3.5), 
                    clip = "off",
                    expand = FALSE) +
    geom_segment(aes(x = -0.2, xend = -0.2, # specifying the vertical arrow
                     y = arrow_start,
                     yend = arrow_end),
                 arrow = arrow(length = unit(0.5,"cm")), size = 1.5, colour = "black") +
        geom_segment(aes(x = -1, xend = -1, 
                     y = -Inf,
                     yend = Inf), alpha = divider_alpha, size = 1) + 
    geom_label(aes(x = 2, y = text_pos), label = paste("Plotted data\n", comparison_text, "than\nalternatives"), fill = "lightgrey") + 
    scale_y_continuous(breaks = seq(lower_lim + 1.5, # breaks where the y-axis labels will be
                                    upper_lim - 1.5,
                                    by = 7),
                       labels = percent_format(scale = 1, accuracy = 1),
                       trans = axis_transform) + # y-axis labels
    my_theme() +
    theme(plot.background = element_rect (fill = fill_colour, color = 'black')) 
  
  
  assign(value = g, envir = .GlobalEnv, paste0("g", 
                                               substr(pos, 1, 1),
                                               substr(orientation, 1, 1)))
  
}

invisible(do.call(mapply, c(create_plot, unname(id))))

ghc +  ggtitle('Conventional') + ylab("HIGH\nPhysical Position\n\n") +
  ghi + ggtitle('Inverted') + glc + ylab("LOW\nPhysical Position\n\n") + gli 

```

Research suggests charts with inverted axes are prone to misinterpretation. Risch (2008) argues that comprehension of data visualisations is facilitated when data is plotted in a manner consistent with common visual metaphors (where greater amounts are associated with higher vertical positions). In an empirical study, 96% of participants correctly interpreted the pattern of data in line charts with conventional y-axis orientation, compared to just 18% for line charts with inverted y-axes (Pandey et al., 2015). Woodin et al. (2021) also found that trends are correctly identified less frequently in charts with inverted y-axes than in charts with conventional y-axes (HOW MUCH?). In these studies, participants were not informed that they were viewing inverted charts, in this study they are given explicit instruction.

Predictions from pre-registrations

## Method

### Design

Latin-squared design. Each participant was exposed to every combination of position and axis orientation conditions, but unlike E1, each participant only saw one version of each scenario.

### Materials

To provide some compensation for the reduced experimental power caused by a reduction in participant numbers, and a shift from a repeated-measures to a Latin-squared design (reducing the number of observations per participant), we increased the number of experimental items for this experiment. Three scenarios which were fillers in E1 were used as experimental items, and one extra item was added for use as an experimental item, and one filler item was replaced due to concerns about its quality (it concerned the risk to others as well as the risk to oneself). The sampling mean of the data was changed for X items in order to present more realistic datasets. E2 used a total of 24 experiment items, 12 filler items and 5 attention check questions (41 trials total).

### Participants

120 participants

### Procedure

The procedure was the same as E1, except for the slide in the instructions, which encouraged participants to pay attention to axis orientation. INCLUDE TEXT HERE

## Analysis

Due to experimenter error, data from one participant was collected prior to pre-registration, but was not analysed.

### Visualisations

```{r r2-p-plot, echo=FALSE, fig.cap="Participants rated the probability of each negative event occurring on a 7-point Likert scale. The distribution of ratings is shown separately for each combination of the levels of each condition (axis orientation: conventional, inverted; data points' position: high, low). Note that the pattern of responses to data presented at different positions in the Conventional Axis condition appears to be the opposite to the pattern for Inverted Axis condition. When charts used conventional axes, higher probability ratings were more common in the High Position condition, whereas when charts used inverted axes, higher probability ratings were more common in the Low Position condition."}
likert_plot(risk2_tidy) + facet_wrap(~ ori, ncol = 1, labeller = labeller(ori = str_to_title))
```

## Main Effects

### Probability Ratings

```{r r2-p, cache=TRUE, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE}
r2_p <- buildclmm(chance_slider.response ~ pos*ori +
                        (1 + pos*ori | participant) +
                        (1 + pos*ori | item_no),
                      data = risk2_tidy)
```

```{r r2-p-cmpr, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r2-p"}
r2_p_cmpr <- clmm(comparison(r2_p),
             data = risk2_tidy)
```

```{r r2-p-anova, echo=FALSE, include=FALSE}
anova_results(r2_p, r2_p_cmpr)
```

For probability magnitude ratings, there is a significant interaction between position in physical space, and y-axis orientation (\chi\^2)(`r in_paren(r2_p.df)`) = `r printnum(r2_p.LR)`, p `r printp(r2_p.p, add_equals = TRUE)`.

```{r r2-p-emms, echo=FALSE, include=FALSE}
r2_p_emm <- emmeans(r2_p@model, ~ pos * ori)

id <- expand_grid(c("hi", "lo"), c("conventional", "inverted"))

get_emms <- function(position, orientation) {
  
  mm <- r2_p_emm %>%
    as_tibble() %>%
    filter(pos == position & ori == orientation) %>% 
    pull(emmean)
  
  assign(value = mm, 
         envir = .GlobalEnv, 
         paste0("r2_p_emm.",
                substr(position, 1, 1),
                substr(orientation, 1, 1)))

}

invisible(do.call(mapply, c(get_emms, unname(id))))
```

```{r r2-int-plot, echo=FALSE, fig.cap="Estimated marginal means (generated by our statistical model) for responses to data points at high and low physical positions, in charts with conventional and inverted axes. \n The slope for conventional charts differs from the slope of inverted charts. Thus, the effect of position on interpretation of magnitudes differs according to axis orientation. Translucent bars show 95% confidence intervals."}
r2_p_emm %>%
  as_tibble() %>%
  mutate_at(vars("emmean":"asymp.UCL"), as.numeric) %>%
  ggplot(aes(x = pos, y = emmean, colour = ori)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL), 
                 position = position_dodge(width = 0.2),
                 size = 3, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.2), size = 3) +
  geom_line(aes(group = ori), 
            position = position_dodge(width = 0.2), 
            size = 2) +
  lims(y = c(-1.65, 2)) + 
  labs(y = "Est. Marginal Mean",
       x = "Physical Position",
       colour = "Orientation") +
  scale_x_discrete(labels = c('Low','High'),
                   limits = c("lo", "hi")) +
  scale_colour_brewer(labels = c('Conventional', 'Inverted'),
                      limits = c("conventional", "inverted"),
                      palette = "Set2") + 
  theme_minimal() +
  theme(legend.position = "top")
```

```{r get-contrasts-function, echo=FALSE}
get_contrasts <- function(orientation, position, contrast_df) {

  params <- c("estimate", "SE", "df", "z.ratio", "p.value")

    one_row <- eval(parse(text = contrast_df)) %>%
      as_tibble() %>%
      filter(ori == orientation & pos == position)

    get_cols <- function(param) {

      if (position == ".") 
      assign(value = one_row %>% pull(param), 
                     envir = .GlobalEnv, 
                     paste0(contrast_df,
                            ".",
                            substr(orientation, 1, 1),
                            ".", 
                            param))
        else #(orientation == ".")
          assign(value = one_row %>% pull(param), 
                 envir = .GlobalEnv, 
                 paste0(contrast_df,
                        ".",
                        substr(position, 1, 1),
                        ".",
                        param))
      
    }
      
     lapply(params, get_cols)

}
```

```{r r2-p-contrasts, echo=FALSE, include=FALSE}
r2_p_contrast <- contrast(r2_p_emm, "consec", simple = "each", combine = TRUE, adjust = "sidak")

  id <- cbind(c(".", ".", "conventional", "inverted"), c("hi", "lo", ".", "."), rep("r2_p_contrast")) %>% as_tibble(.name_repair = "unique")

invisible(do.call(mapply, c(get_contrasts, unname(id))))
```

Pairwise comparisons (with Sidak adjustment) reveal that the main effect of position in charts with conventional y-axis orientations (E1) is replicated (z = `r printnum(r2_p_contrast.c.z.ratio)`, p `printp(r2_p_contrast.c.p.value, add_equals = TRUE)`). Data points at higher positions were associated with higher probability ratings than data points at lower positions.

There was no significant difference between ratings for data points plotted at different positions when inverted axes were used (`r printnum(r2_p_contrast.i.z.ratio)`, p `printp(r2_p_contrast.i.p.value, add_equals = TRUE)`). The effect of position on ratings seems to disappear when an inverted axis is used. We do not observe the same pattern of results when an inverted axis is used instead of a conventional axis.

Data points plotted at high physical positions were associated with higher ratings when a conventional axis was used than when an inverted axis was used (`r printnum(r2_p_contrast.h.z.ratio)`, p `printp(r2_p_contrast.h.p.value, add_equals = TRUE)`). However, data points plotted at low physical positions were not associated with different ratings depending on the axis orientation (`r printnum(r2_p_contrast.l.z.ratio)`, p `printp(r2_p_contrast.l.p.value, add_equals = TRUE)`.

This suggests that differences in ratings for data points at different positions in physical space are not due to simple associations between vertical position and magnitude.

### Severity Ratings

```{r r2-s, cache=TRUE, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE}
r2_s <- buildclmm(severity_slider.response ~ pos*ori +
                        (1 + pos*ori | participant) +
                        (1 + pos*ori | item_no),
                      data = risk2_tidy)
```

```{r r2-s-cmpr, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r2-s"}
r2_s_cmpr <- clmm(comparison(r2_s),
             data = risk2_tidy)
```

```{r r2-s-anova, echo=FALSE, include=FALSE}
anova_results(r2_s, r2_s_cmpr)
```

```{r r2-s-emm, echo=FALSE, include=FALSE}
r2_s_emm <- emmeans(r2_s@model, ~ pos * ori)

r2_s_contrast <- contrast(r2_s_emm, "consec", simple = "each", combine = TRUE, adjust = "sidak")

  id <- cbind(c(".", ".", "conventional", "inverted"), c("hi", "lo", ".", "."), rep("r2_s_contrast")) %>% as_tibble(as_tibble(.name_repair = "unique"))
  
invisible(do.call(mapply, c(get_contrasts, unname(id))))
```

There was also an interaction between physical position and axis orientation for severity magnitude ratings: (\chi\^2)(`r in_paren(r2_s.df)`) = `r printnum(r2_s.LR)`, p `r printp(r2_s.p, add_equals = TRUE)`. However, the main effect from E1, different responses to data points at different positions in conventional graphs, was not replicated (`r printnum(r2_s_contrast.c.z.ratio)`, p `r printp(r2_s_contrast.c.p.value, add_equals = TRUE)`. There was also no evidence of different responses to data points at different positions in inverted charts (`r printnum(r2_s_contrast.i.z.ratio)`, p `r printp(r2_s_contrast.i.p.value, add_equals = TRUE)`). This interaction appears to be driven by a weak and likely spurious difference between ratings for data points at high positions in inverted and conventional charts (`r printnum(r2_s_contrast.h.z.ratio)`, p `r printp(r2_s_contrast.h.p.value, add_equals = TRUE)`).

```{r r2-pl, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r2-p"}
r2_pl <- clmm(add.terms(formula(r2_p), "literacy"),
              data = risk2_tidy)
```

```{r r2-sl, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r2-s"}
r2_sl <- clmm(add.terms(formula(r2_s), "literacy"),
              data = risk2_tidy)
```

```{r r2-pl-summary, echo=FALSE, include=FALSE}
summary_extract(r2_pl, "pos1:ori1")
```

```{r r2-sl-summary, echo=FALSE, include=FALSE}
summary_extract(r2_sl, "ori1:pos1")
```

Probability: z = `r printnum(r2_pl.statistic)`, p `r printp(r2_pl.p.value, add_equals = TRUE)`.

Severity: z = `r printnum(r2_sl.statistic)`, p `r printp(r2_sl.p.value, add_equals = TRUE)`.

The interaction term remained significant when controlling for data visualisation literacy.

This represents a divergence from pre-registered analysis protocol, to match the simpler moderation/covariate use of literacy in E1.

## Discussion

When using conventional graphs only (E1), we found that displaying data within different axis limits affected magnitude judgements. However, it was unclear whether participants' judgements were based on data points' absolute or relative positions, because both would evoke similar interpretations. So, in this experiment, for half of trials, we reversed the mapping of values in physical space so these two features would imply different magnitudes for a given value.

In the 2x2 experiment, we replicated the primary finding from E1. In charts with conventional axis orientations, the same data points elicited different magnitude judgements when presented at different positions. These judgements corresponded to magnitudes implied by data points' absolute and relative positions. However, in charts with inverted axis orientations, the same pattern was not observed. Therefore, we can conclude that interpretations of magnitude are affected by a chart's physical arrangement of values. Differences in magnitude judgements for the same data points at contrasting physical positions are heterogenous if axes are oriented differently. This interaction was robust to inclusion of graph literacy as an additional effect.

However, there was not significant evidence to suggest that the same data points elicited different magnitude judgements when presented at different positions in *inverted* charts. Therefore, we can't conclude from these data that magnitude judgements are solely driven by data points' positions relative to other values. However, \@ref(fig:r2-int-plot) suggest that the pattern of results for inverted charts is the reverse of the pattern for conventional charts. An additional experiment is required to confirm whether this is the case.

# Experiment 3

## Introduction

The significant interaction in E2 revealed that the influence of position on magnitude judgements depends on how a chart arranges different numerical values (axis orientation). The pattern of responses in inverted charts appeared to be the inverse of the pattern for conventional charts. So, participants did not just make simple inferences about magnitude based on conceptual metaphors, but paid attention to context. However, the absence of a significant difference between high and low data points in the inverted charts prohibits the conclusion that interpretations are driven entirely by comparisons with other parts of a chart.

It is possible that no significant effect was detected due to insufficient experimental power. Unlike E1 with 150 participants in repeated-measures single-factor design, E2 involved 120 participants in a Latin-squared 2x2 design. Despite an increase in the number of experimental items (from 20 to 24), there were still fewer observations for each unique condition (3000 in E1 vs. 720 in E2).

Therefore, it is appropriate to test, in an experiment with increased experimental power, whether magnitude is interpreted differently for the same data points at different positions in inverted charts. This will provide a clearer account of the true nature of the interaction in E2, furthering understanding of the mechanism which causes interpretations of magnitude to be affected by data points positions.

## Method

### Materials

Materials were identical to E1, except for the inversion of the y-axis in all charts, including practice items. There were 60 trials in total (40 experimental, 15 fillers, 5 attention check questions).

### Participants

One-hundred and fifty individuals participated in this experiment. The experiment was not made available to those who had participated in Experiments 1 and 2 or those who signed-up to prolific.co prior to 24th July 2021. A viral social media post on 24th July 2021 endorsing the website attracted many new users from a narrow demographic, skewing studies' participant distributions (Prolific, 2021).

### Procedure

One slide in the instructions explained to participants how charts with inverted axes function: *"In all graphs in this experiment, the arrow on the 'Chance' axis points downwards, meaning the numbers get bigger as the axis goes down."*. Otherwise, the procedure was identical to Experiments 1 and 2.

### Design

Like E1, this experiment used a within-participants repeated-measures design: each participant responded to both versions of each chart.

## Analysis

```{r r3-p-plot, echo=FALSE, fig.cap="Participants rated the probability of each negative event occurring on a 7-point Likert scale. The distribution of ratings, ranging from \"Very unlikely\" (far left, dark green) to \"Very likely\" (far right, red), is shown separately for charts where values were presented at a High Position (top) and a Low Position (bottom). Note that values in the Low Position condition were more frequently rated as representing higher probabilities (right-hand side) than values in the High Position condition, which were more frequently rated as representing lower probabilities (left-hand side)."}
likert_plot(risk3_tidy)
```

```{r r3-p, cache=TRUE, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE}
r3_p <- buildclmm(chance_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = risk3_tidy)
```

```{r r3-p-cmpr, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r3-p"}
r3_p_cmpr <- clmm(comparison(r3_p),
             data = risk3_tidy)
```

```{r r3-p-anova, eval=TRUE, echo=FALSE}
anova_results(r3_p, r3_p_cmpr)
```

A likelihood ratio test reveals a significant difference between probability magnitude ratings for data points at different positions in inverted charts: (\chi\^2)(`r in_paren(r3_p.df)`) = `r printnum(r3_p.LR)`, p `r printp(r3_p.p, add_equals = TRUE)`. Ratings were higher for data points at lower positions.

```{r r3-s, cache=TRUE, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE}
r3_s <- buildclmm(severity_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = risk3_tidy)
```

```{r r3-s-cmpr, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r3-s"}
r3_s_cmpr <- clmm(comparison(r3_s),
             data = risk3_tidy)
```

```{r r3-s-anova, eval=TRUE, echo=FALSE}
anova_results(r3_s, r3_s_cmpr)
```

There was insufficient evidence for difference between severity magnitude ratings in inverted charts: (\chi\^2)(`r in_paren(r3_s.df)`) = `r printnum(r3_s.LR)`, p `r printp(r3_s.p, add_equals = TRUE)`.

```{r r3-pl, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r3-p"}
r3_pl <- clmm(add.terms(formula(r3_p), "literacy"),
              data = risk3_tidy)
```

```{r r3-sl, cache=TRUE, eval=TRUE, echo=FALSE, warning=FALSE, dependson="r3-s"}
r3_sl <- clmm(add.terms(formula(r3_s), "literacy"),
              data = risk3_tidy)
```

```{r r3-pl-summary, echo=FALSE, include=FALSE}
summary_extract(r3_pl, "condition1")
```

```{r r3-sl-summary, echo=FALSE, include=FALSE}
summary_extract(r3_sl, "condition1")
```

Probability: z = `r printnum(r3_pl.statistic)`, p `r printp(r3_pl.p.value, add_equals = TRUE)`

Severity: z = `r printnum(r3_sl.statistic)`, p `r printp(r3_sl.p.value, add_equals = TRUE)`

## Discussion

When viewing charts with inverted axes, participants judged data points' magnitude according to whether accompanying blank space implied the existence of higher or lower plausible values. They ignored conventional associations between position and magnitude to interpret magnitude in the context of the chart.

```{r r3-p-emms, echo=FALSE, eval = TRUE}
r3_p_emm <- emmeans(r3_p@model, ~ condition) %>% as_tibble()

r3_p_emm.h <- r3_p_emm %>% filter(condition == "hi") %>% pull(emmean)
r3_p_emm.l <- r3_p_emm %>% filter(condition == "lo") %>% pull(emmean)
```

In the previous experiment (E2), we did not observe a significant difference between magnitude ratings for data points at different positions, for inverted graphs. We suggested that insufficient experimental power to detect this difference in E2, may have resulted in a false negative. However, increased power in E3 provides greater confidence in a genuine significant effect.

E2 involved switching between conventional and inverted charts, whereas E3 presented inverted charts in isolation. However, estimated marginal means, which capture the difference in ratings for values at different positions, are almost identical across these two experiments (E2: `r printnum(abs(r2_p_emm.hi-r2_p_emm.li))`; E3: `r printnum(abs(r3_p_emm.h-r3_p_emm.l))`), suggesting these charts were not treated differently in the different experiments. Therefore, switching should not prohibit the use of E3's data to reinterpret the interaction in E2

In light of this, we can re-interpret the significant interaction in E2. The same data points, presented at the same positions in a chart, will convey different magnitudes depending on how they compare to plausible values implied by blank space. Viewers do not draw upon simple associations between vertical position and magnitude, but recognise the context in which values are plotted.

# General Discussion

Over three experiments, we demonstrate how judgements of the magnitude of data points are influenced by the presence of blank space in a chart. Regardless of their physical positions, data points were associated with greater magnitudes when they were numerically greater than the plausible values represented by blank space. This highlights viewers' sensitivity to context in interpretation of information in data visualisations, suggesting designers should consider this aspect when creating charts.

When comparing data points within a single chart, it is appropriate to infer that data points which appear at different positions between two axis limits have distinct magnitudes. This study's contribution is demonstrating that magnitude judgements vary when *the same value* appears at different positions between two axis limits. Interpretation of an absolute value is biased by its relative position. 

The impact of surrounding context on assessments of data is an example of a framing effect. We illustrate that this effect occurs without explicitly presenting contrasting points. The presence of blank space is sufficient for implying the relative status of plotted data. This bears similarities to Lundqvist et al.'s (year) observation that the language used to qualify a number affects interpretations of its magnitude, even without any reference to other numbers.

The present data complement findings from prior research on y-axis truncation, which has found that the choice of axis limits can impact interpretation of data. These results reinforce the notion that the amount of blank space surrounding plotted values influences viewers' impressions of these values. Previous investigations have shown that y-axis values affect judgements associated with comparing values (e.g., Correll et al., 2020; Pandey et al., 2015; Witt, 2019; Yang et al., 2021), the present findings show that they also affect judgements associated with individual values. The pattern of data is consistent with previous research which has found that interpretations of magnitude differ according to surrounding context (Borges & Sawyers, 1974; Brown et al., 2008; Jarvella et al., 1995; Lundquist & Jarvella, 1994). 

A prior study addressing a similar question also concluded that a data point's location within a range of values affects interpretation of its magnitude (Sandman et al. 1994). The present study builds upon this research by identifying the mechanism behind this effect and removing the confound of variable axes ranges. It also extends the finding beyond a single scenario to a wider range of situations, and separately analyses specific judgements, rather than using a combined measure, to verify that different presentations affect judgements of plotted data, not related concepts.

This set of experiments was not concerned with endorsing or opposing use of inverted charts; the sole function of these charts was in distinguishing competing explanations. However, our data provide evidence of comprehension, contrary to the typical misinterpretation resulting from associating higher position with higher values (Woodin et al., 2021, Pandey et al., 2015). This is most likely because explicit instruction was provided. 

Visualisation rhetoric involves presenting numerical information in a way that provokes a particular interpretation (Hullman & Diakopoulos, 2011). The manipulation of visualisation components examined in the present study are related to two rhetorical strategies: axis thresholding and contrast. The former is an instance of information access rhetoric, and involves setting an axis range that provides an incomplete picture of the data. The latter is an instance of a mapping rhetoric, and employs visual properties to promote comparisons. 

We did not find consistent evidence that assessments of outcome severity are affected by the positioning of values representing event probability. This contrasts with prior research on the interplay between appraisals of probability and event magnitude which reports that probability estimates change as a function of magnitude (Harris & Corner, 2011; Harris et al., 2009) and that magnitude estimates change as a function of event probability (Kupor & Laurin, 2020). In contrast to this prior work, which substantially manipulated underlying scenarios, our more subtle manipulation retained the same probability value, changing only surrounding context. The effect of relative position on interpretation of numerical information does not consistently extend to magnitude judgements about related but distinct concepts.

Adjusting for data visualisation literacy did not invalidate findings regarding the influence of axis range on interpretation. Yang et al. (2021) also observed that data visualisation literacy could not sufficiently explain variance in the degree of bias caused by y-axis truncation. This measure captures comprehension of the conventions of data visualisation, indicating receipt of elementary instruction (Okan et al., 2016). Therefore, it is perhaps better suited to capturing the ability to decipher more complicated designs, but is not well-placed to predict susceptibility to differences in presentation format (Yang et al., 2021).

## Implications for Visualisation Design

This finding highlights an opportunity for data visualisation designers to creatively construct axes for dramatic effect. Setting axis limit to introduce blank space allows designers to persuasively convey large or small magnitudes. However, even those avoiding creative use of blank space should pay regard to this finding. Axis ranges are likely to be considered representative of the relevant values for assessing the magnitude of plotted data. Designers should reflect on the impression(s) of magnitude resulting from their choice of axis limits. To avoid misleading displays, axes should present appropriate values. Like Correll et al. (2020), we acknowledge that there is no objectively correct method to achieve this. Ultimately, the designer decides what context is appropriate, based on the chart's purpose and content. This may involve taking account of historical data, comparable scenarios, established baselines, current objectives, etc.. This research is also relevant when assessing the quality of data visualisations; one should consider appropriate portrayal of magnitude in addition to standard considerations.

Setting an axis range that extends far beyond the range of the plotted data impacts discrimination ability (Witt, 2019), and may distract attention from meaningful variance within the data. Witt recommends setting an axis range to 1.5-2 times the standard deviation of the dataset. This guidance is broadly consistent with our suggestions in its recommendation that axis limits should take into account relevant values to provide context. The present experiment has demonstrated that magnitude is communicated by the relative position of data points within the space of all plausible values.

Under Witt's guidance, data points' positions are determined solely by the size of the difference between two conditions. A large difference between conditions would result in data points being located near the two extremes of the chart, which may capture genuine small and large magnitudes. At other times, applying Witt's guidance will create an inaccurate impression of individual magnitudes. For example, with a small difference between conditions, no data points will be displayed near the extremes, even though they may be genuinely large or small when considered within a larger context. This occurs because Witt's guidance was created for the sole purpose of managing bias and sensitivity when comparing two conditions (in fields with standardised effect sizes). Accordingly, setting axes which provide context for *individual* magnitudes, is not considered pertinent. Again, designers must consider their dataset and the message they intend to relate in order to reach a trade-off between suitable communication of variability and of individual magnitudes.

A possible compromise, may involve displaying values against blank space to convey magnitude in context and also in a focused context to facilitate comparisons between values. This resembles an approach for communicating differences suggested by Correll et al. (2020), and reported to benefit users by Ritchie et al.'s (2019). Its suitability for conveying magnitude should be investigated in future work.

## Limitations

To avoid likelihood of misinterpretation, participants were given instructions on how to read inverted charts. This may have suppressed a spontaneous interpretation, based on physical position, in favour of a learnt interpretation. Our study therefore only explains how users interpret magnitude when they know how to interpret the chart itself. 

In addition to associations between vertical position and magnitude, a common conceptual metaphor for emotional valence also uses vertical position. Lower physical positions are typically associated with negative valence and higher physical positions with positive valence. Woodin et al. (2021) found that comprehension is facilitated when the physical arrangement of data is consistent with the conceptual metaphor for valence, but associations between vertical position and magnitude affect interpretation more strongly. In the present set of experiments, charts displayed negative outcomes, so data was aligned with the conceptual metaphor for valence in inverted charts, and misaligned in conventional charts. Participants evidently did not use valence metaphors to interpret values in conventional charts - that would have produced the opposite pattern of results to those observed. So, the simplest explanation for our data is that participants relied on relative position when interpreting both conventional and inverted charts, rather than some of the time generating inferences based on a conceptual metaphor for valence.

In analyses employing graph literacy as a covariate, graph literacy scores were calculated as the average of five Likert scale responses. This means that responses to graph literacy questions were modelled as continuous data, whilst Likert scale responses to the probability and severity questions were modelled as ordinal data,. This approach was used by the scale's developers (Garcia-Retamero et al., 2016), but is unlikely to be the most appropriate method.

## Conclusion
